---
title: "Tornado analytics: frequency"
format: 
  revealjs:
    code-fold: false
    theme: white
    footer: "GEO4251 Climate Change & Storms"
    chalkboard: true
editor: source
editor_options: 
  chunk_output_type: console
---

## Topics

-   Shifting location
-   More clustering
-   Stronger

##

![Tornado tracks](https://farm8.staticflickr.com/7090/7157010997_2a92fa603c_o.jpg)

## 

-   Where do you find these historical data on tornadoes?
-   How do you get the data into a format you can work with?
-   The Storm Prediction Center has data on severe local storms in the U.S. back to 1950. The data are available [here](https://www.spc.noaa.gov/wcm/)
-   The European Severe Storms Laboratory has data on severe local storms across Europe. The data are available [here](https://www.eswd.eu/?)

## {.smaller}

-   For this lesson you are interested in the file called `1950-2021_actual_tornadoes.csv`
-   First download the file from the site with the `download.file()` function specifying the location (`url =`) and a name you want the file to be called on your computer (`destfile =`)

```{r, eval=FALSE}
#| echo: true
L <- "http://www.spc.noaa.gov/wcm/data/1950-2021_actual_tornadoes.csv"
download.file(url = L,
              destfile = here::here("data", "Tornadoes.csv"))
```

-   A file called `Tornadoes.csv` should now be located in the directory `data`
-   Click on the *Files* tab in the lower-right panel, then select the `data` folder

##  {.smaller}

-   Import the file as a data frame using the `readr::read_csv()` function and then preview it with the `head()` function

```{r}
#| echo: true
Torn.df <- readr::read_csv(file = here::here("data", "Tornadoes.csv"),
                           show_col_types = FALSE)

Torn.df |>
  head()
```

-   Each row is a tornado report
-   Observations include the day and time, the state (`st`), the maximum EF rating (`mag`), the number of injuries (`inj`), the number of fatalities (`fat`), estimated property losses (`loss`), estimated crop losses (`closs`), start and end locations in decimal degrees longitude and latitude, length of the damage path in miles (`len`), width of the damage in yards (`wid`)

## 

![Birmingham, AL tornado](figures/BirminghamDamagePath.jpg)

## Damage rating

[EF rating](https://en.wikipedia.org/wiki/Enhanced_Fujita_scale)


##

-   The number of tornado reports in the data set is returned using the `nrow()` function

```{r}
#| echo: true
Torn.df |>
  nrow()
```

-   67,558 tornado reports since 1950

## {.smaller}

-   Tornado frequency by state

```{r}
#| echo: true
ST_Counts.df <- Torn.df |>
  dplyr::group_by(st) |>
  dplyr::summarize(nT = dplyr::n())

ST_Counts.df |>
  head()
```

## {.smaller}

-   Create a choropleth map
-   Get the state boundaries and shift the geometry of Alaska, Hawaii, and Puerto Rico
-   Select only the column with state abbreviation `stusps` and change the column name to `st`
-   Join with the counts data on the column name `st`

```{r}
#| echo: true
library(sf)

States.sf <- USAboundaries::us_states() |>
  tigris::shift_geometry() |>
  dplyr::select(st = stusps) |>
  dplyr::left_join(ST_Counts.df, by  = "st")

States.sf |>
  head()
```

##

```{r}
#| echo: true
library(ggplot2)

ggplot(data = States.sf,
       mapping = aes(fill = nT)) +
  geom_sf(color = "lightgrey", size = .1)
```

##

-   Annual rate per 100 square kilometers (km^2)

```{r}
#| echo: true

States.sf <- States.sf |>
  dplyr::mutate(Area = sf::st_area(geometry) / 10^10,
                Area = units::drop_units(Area),
                AnnualRate = nT / Area / 72)

ggplot(data = States.sf,
       mapping = aes(fill = AnnualRate)) +
  scale_fill_viridis_c(option = "cividis") +
  geom_sf(color = "lightgrey", size = .1) +
  labs(title = "Annual tornado rate: 1950-2021 (per 100,000 sq. km)", 
       fill = "", caption = "Data source: NOAA SPC")
```

## {.smaller}

-   Time series comparing the frequency of tornadoes in Tornado alley and Dixie alley
-   Count by states and year

```{r}
#| echo: true
Counts.df <- Torn.df |>
  dplyr::filter(mag >= 2) |>
  dplyr::mutate(Alley = dplyr::case_when(st %in% c("KS", "OK") ~ "Tornado Alley",
                                          st %in% c("MS", "AL") ~ "Dixie Alley",
                                          TRUE ~ "Outside")) |>
  dplyr::group_by(Alley, yr) |>
  dplyr::summarize(nT = dplyr::n())

Counts.df |>
  head()
```

##

```{r}
#| echo: true
#| output-location: slide
Counts.df |>
  dplyr::filter(!Alley == "Outside") |>
ggplot(mapping = aes(x = yr, y = nT, color = Alley)) +
  geom_point() + geom_line() +
  geom_smooth(method = glm, 
              method.args = list(family = 'poisson'), se = FALSE) +
  scale_color_manual(values = c("#b87333", "#478ccc")) +
  scale_x_continuous(breaks = seq(1950, 2020, by = 10)) +
  labs(x = "", y = "", color = "",
       title = "Decreasing number of tornadoes rated as causing significant damage",
       subtitle = "Annual number of (E)F2+ tornadoes originating in portions of Dixie Alley (MS, AL) and\n Tornado Alley (OK, KS)", 
       caption = "Data source: NOAA SPC") + theme_minimal()
```

##

-   Correlation in annual counts

```{r}
#| echo: true
cor(Counts.df$nT[Counts.df$Alley == "Tornado Alley"],
    Counts.df$nT[Counts.df$Alley == "Dixie Alley"])
```

##

-   Total annual number of tornadoes by EF rating

```{r}
AnnualCounts.df <- Torn.df |>
  dplyr::filter(mag >= 1 & mag <= 4) |>
  dplyr::group_by(yr, mag) |>
  dplyr::summarize(nT = dplyr::n()) |>
  dplyr::rename(Year = yr)

ggplot(data = AnnualCounts.df,
       mapping = aes(x = yr, y = nT)) +
  geom_line() +
  geom_smooth() +
  facet_wrap(~ mag, scales = "free_y")
```

##

-   Compare with ENSO

```{r}
#| echo: true
oni.df <- readr::read_csv(here::here("data", "ONI.csv")) |>
  data.frame() |>
  dplyr::mutate(Date = as.Date(paste0(Year, "-", Month, "-01")))
```

-   Annual average ONI

```{r}
AnnualONI.df <- oni.df |>
  dplyr::group_by(Year) |>
  dplyr::summarize(AvgONI = mean(ONI_C))
```

##

-   Join with tornado counts

```{r}
AnnualCounts.df <- AnnualCounts.df |>
  dplyr::left_join(AnnualONI.df, by = "Year")
```

-   Annual correlations by EF rating

```{r}
AnnualCounts.df |>
  dplyr::filter(Year > 1985) |>
  dplyr::group_by(mag) |>
  dplyr::summarize(r = cor(nT, AvgONI))
```

## Daily sunspot numbers

-   Sunspot numbers <https://www.sidc.be/silso/datafiles#total>

```{r}
#| echo: true
ssn.df <- readr::read_table(file = here::here("data", "Sunspots.txt"),
                            col_names = c("Year", "Month", "Day", "YearF", 
                                          "SSN", "SSN_error", "Nobs", "X")) |>
  dplyr::filter(Year >= 1950, Year <= 2021) |>
  dplyr::mutate(date = as.Date(paste0(Year, "-", Month, "-", Day)))
```

##

-   Big tornado days and average sunspot numbers

```{r}
#| echo: true
Combined.df <- Torn.df |>
  dplyr::filter(mo %in% c("05", "06", "07", "08")) |>
  dplyr::group_by(date) |>
  dplyr::summarize(nT = sum(mag >= 1)) |>
  dplyr::left_join(ssn.df, by = "date")
```

##

```{r}
#| echo: true
Combined.df |>
  dplyr::mutate(Big = nT > 5) |>
  dplyr::group_by(Big) |>
  dplyr::summarise(AvgSSN = mean(SSN),
                   nDays = dplyr::n(),
                   se = sd(SSN)/sqrt(nDays))
```

##

```{r}
Yearly.df <- Combined.df |> 
  dplyr::group_by(Year) |>
  dplyr::summarize(AvgSSN = mean(SSN),
                   nT = sum(nT))
```

##

-   <http://coolwx.com/record/>
-   lapse rates on SkewT-logP charts from <https://www.tropicaltidbits.com/>
